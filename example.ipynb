{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Downloading reddit comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RWV.pushshift.get_data_threading import GetContent, timestamp_to_utc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_comments(after, before, subreddit, save_file_name):\n",
    "    comments = GetContent(after=after, before=before, subreddit=subreddit, content='comment', \n",
    "                          thread_num=4, max_per_sec=1, log_level='info')\n",
    "    comments.get_content()\n",
    "    comments.save_content(save_file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "days = 7\n",
    "delta_t = 24*60*60\n",
    "\n",
    "now_time = time.time()\n",
    "t2 = now_time\n",
    "\n",
    "for day in range(days):\n",
    "    t1 = now_time - (day + 1)*delta_t\n",
    "    \n",
    "    print('getting comments from {} to {}'.format(timestamp_to_utc(t1), timestamp_to_utc(t2)))\n",
    "    \n",
    "    get_comments(t1, t2, 'askreddit', 'askreddit_{}'.format(day+1))\n",
    "    t2 = t1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking what was downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = os.path.abspath('') + '/RWV/data/reddit_data'\n",
    "file_lst = os.listdir(path)\n",
    "\n",
    "saved_reddit = []\n",
    "for file in file_lst:\n",
    "    if file != '.gitignore':\n",
    "        saved_reddit.append(file)\n",
    "\n",
    "saved_reddit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RWV.pushshift.load_data import Content\n",
    "\n",
    "def load_saved(file_name):\n",
    "    f = Content(file_name)\n",
    "    comments = f.load_comments()     # load comments as list of Comment objects\n",
    "    return comments\n",
    "\n",
    "comments = load_saved(saved_reddit[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "small_comments = load_saved(saved_reddit[0])[0:5]\n",
    "for comment in small_comments:\n",
    "    print('author: {}\\ntext: {}\\ntime: {}\\n\\n'.format(comment.author,\n",
    "                                                      comment.body, timestamp_to_utc(comment.created_utc)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RWV.text_processing.process_reddit import word2vec_input, count_words\n",
    "\n",
    "w2v_input = word2vec_input(comments)\n",
    "\n",
    "print('word2vec input example:\\n', word2vec_input(small_comments), '\\n')\n",
    "\n",
    "word_count = count_words(w2v_input)\n",
    "most_common = sorted(word_count, key=word_count.get, reverse=True)[:20]\n",
    "\n",
    "for w in most_common:\n",
    "    print('{}: {}'.format(w, word_count[w]))\n",
    "\n",
    "count = 0\n",
    "for c in w2v_input:\n",
    "    count += len(c)\n",
    "\n",
    "print('\\ntotal sentences: {}, total words: {}'.format(len(w2v_input), count))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector models\n",
    "This is just an example how things work and that they work. Testing was done only with 2 days worth of r/askreddit data with min_count=50 which is to big for this small example (lots of \"common\" missing words)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from RWV.vector_model.word_embeddings import WordEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "saved_reddit = [saved_reddit[0], saved_reddit[2]]  # use only 2 days of data\n",
    "\n",
    "w2v_model = WordEmbedding(model_type='word2vec')\n",
    "w2v_model.make_model(sentences=saved_reddit, content='comment', epochs=5, size=300, window=5,\n",
    "                     min_count=50, workers=8)\n",
    "w2v_model.save_model('w2v_askreddit.kv')\n",
    "\n",
    "print('finished in: {}'.format(time.time() - start))\n",
    "\n",
    "# finished in: 377.2292971611023\n",
    "# file size: 15.4 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FastText model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "saved_reddit = [saved_reddit[0], saved_reddit[2]]\n",
    "\n",
    "ft_model = WordEmbedding(model_type='fasttext')\n",
    "ft_model.make_model(sentences=saved_reddit, content='comment', epochs=5, size=300, window=5,\n",
    "                     min_count=50, bucket=1000000, workers=8)\n",
    "ft_model.save_model('ft_askreddit.kv')\n",
    "\n",
    "print('finished in: {}'.format(time.time() - start))\n",
    "\n",
    "# finished in: 455.5157127380371\n",
    "# file size: 1.2 GB  ->  reduce vector size or buckets for smaller file size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doc2vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "saved_reddit = [saved_reddit[0], saved_reddit[2]]\n",
    "\n",
    "ft_model = WordEmbedding(model_type='doc2vec')\n",
    "ft_model.make_model(sentences=saved_reddit, content='comment', epochs=5, size=300,\n",
    "                     window=5, min_count=50, workers=8)\n",
    "ft_model.save_model('d2v_askreddit.kv')\n",
    "\n",
    "print('finished in: {}'.format(time.time() - start))\n",
    "\n",
    "# finished in: 566.7566244602203\n",
    "# file size: 15.4 MB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing word vector models\n",
    "https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v = WordEmbedding('word2vec').load_model('w2v_askreddit.kv')\n",
    "ft = WordEmbedding('fasttext').load_model('ft_askreddit.kv')\n",
    "d2v = WordEmbedding('doc2vec').load_model('d2v_askreddit.kv')\n",
    "\n",
    "models = [w2v, ft, d2v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in models:\n",
    "    print(m.vocab['cat'].count)\n",
    "    print(len(m.vocab))\n",
    "    print(type(m['cat']))\n",
    "    print(m.similarity('cat', 'dog'))\n",
    "    print(m.most_similar(['cat'], topn=3))\n",
    "    print(m.most_similar(positive=['car', 'cat'], topn=3))\n",
    "    print(m.doesnt_match(['fire', 'water', 'cat']))\n",
    "    print(m.n_similarity(['cat', 'bird'], ['dog', 'fish']))\n",
    "    print(m.wmdistance(['cat', 'bird'], ['dog', 'fish']))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
